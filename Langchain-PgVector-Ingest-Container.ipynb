{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3420575b-4d00-458b-aa0e-7030008ccd53",
   "metadata": {},
   "source": [
    "## Creating an index and populating it with documents using PostgreSQL+pgvector\n",
    "\n",
    "Simple example on how to ingest PDF documents, then web pages content into a PostgreSQL+pgvector VectorStore.\n",
    "\n",
    "Requirements:\n",
    "- A PostgreSQL cluster with the pgvector extension installed (https://github.com/pgvector/pgvector)\n",
    "- A Database created in the cluster with the extension enabled (in this example, the database is named `vectordb`. Run the following command in the database as a superuser:\n",
    "`CREATE EXTENSION vector;`\n",
    "\n",
    "Note: if your PostgreSQL is deployed on OpenShift, directly from inside the Pod (Terminal view on the Console, or using `oc rsh` to log into the Pod), you can run the command: `psql -d vectordb -c \"CREATE EXTENSION vector;\"`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308b229-b520-4e82-a783-eb921bb955e7",
   "metadata": {},
   "source": [
    "### Needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e41b41-f60a-4b0f-91a1-cd273b60f21b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -q pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8116e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a82063d-6153-4812-8977-042241736b53",
   "metadata": {},
   "source": [
    "### Base parameters, the PostgreSQL info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be160a11",
   "metadata": {
    "tags": []
   },
   "source": [
    "First we need to build the CONNECTION  like:\n",
    "```python\n",
    "CONNECTION_STRING = \"postgresql+psycopg://user:password@postgresql-server:5432/vectordb\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ad0096f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql+psycopg://testuser:testpwd@localhost:5432/vectordb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the values from the .env file\n",
    "user = \"testuser\"\n",
    "password =\"testpwd\"\n",
    "database = \"vectordb\"\n",
    "#server = \"af651cca01b154fe28a0df0167cad5a7-844854289.us-east-2.elb.amazonaws.com\"\n",
    "server=\"localhost\"\n",
    "# Construct the connection string\n",
    "CONNECTION_STRING = f\"postgresql+psycopg://{user}:{password}@{server}:5432/{database}\"\n",
    "\n",
    "# Print the connection string\n",
    "print(CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74510b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b378259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cc6ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3013cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2734915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b499a49-128c-4be5-903b-76c40771c7bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "600cd763-6ecc-4c77-89c0-47108c31c44e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.pgvector import PGVector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f6785-480e-4519-be4f-8e1738dba4ca",
   "metadata": {},
   "source": [
    "## Initial index creation and document ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "11fceeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cff5f7-c509-48db-90b5-e15815b8b530",
   "metadata": {},
   "source": [
    "#### Document loading from a folder containing PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dde8a4a3-d602-49c6-b4a5-31a76b25a58b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_folder_path = './rhods-doc'\n",
    "loader = PyPDFDirectoryLoader(pdf_folder_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198fe0a-38bf-4cd4-af7d-35b41c645edd",
   "metadata": {},
   "source": [
    "#### Split documents into chunks with some overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "edba4a08-2194-4df1-9091-6f2b596757a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Vector database\\nA vector database management system (VDBMS) or simply vector database or vector store is a\\ndatabase that can store vectors (fixed-length lists of numbers) along with other data items. Vector databases\\ntypically implement one or more Approximate Nearest Neighbor  (ANN) algorithms,[1][2] so that one can\\nsearch the database with a query vector to retrieve the closest matching da tabase records.\\nVectors are mathematical representations of data in a high-dimensional space. In this space, each dimension\\ncorresponds  to a feature of the data, and tens of thous ands of dimensions might be used to represent\\nsophisticated data. A vector's position in this space represents its characteristics. Words, phrases, or entire\\ndocuments, and images, audio, and ot her types of data can all be vectorized.[3]\\nThese feature vectors may be computed from the raw data using machine learning methods such as feature\\nextraction algorithms, word embeddings[4] or deep learning networks. The goal is that semantically similar\", metadata={'source': 'rhods-doc\\\\Vector_database.pdf', 'page': 0})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024,\n",
    "                                               chunk_overlap=40)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a835e2df",
   "metadata": {},
   "source": [
    "#### Cleanup documents as PostgreSQL won't accept the NUL character, '\\x00', in TEXT fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5aefc08d-a4ad-4aad-9120-cfa98b67cbe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for doc in all_splits:\n",
    "    doc.page_content = doc.page_content.replace('\\x00', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7195a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "37a977c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pip install necessary package\n",
    "#%pip install --upgrade --quiet  langchain-openai\n",
    "#%pip install --upgrade --quiet  psycopg2-binary\n",
    "#%pip install --upgrade --quiet  tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735feb13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ae7eae2-c670-4eb5-803b-b4d591fa83db",
   "metadata": {},
   "source": [
    "#### Create the index and ingest the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c056c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install psycopg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62c084b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "47cbf017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"psycopg[binary]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bbb6a3e3-5ccd-441e-b80d-427555d9e9f6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "COLLECTION_NAME = \"documents_test\"\n",
    "\n",
    "db = PGVector.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca4ca1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is vector database?\"\n",
    "docs_with_score = db.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8437c826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Score:  0.18316369157053736\n",
      "Vector database\n",
      "A vector database management system (VDBMS) or simply vector database or vector store is a\n",
      "database that can store vectors (fixed-length lists of numbers) along with other data items. Vector databases\n",
      "typically implement one or more Approximate Nearest Neighbor  (ANN) algorithms,[1][2] so that one can\n",
      "search the database with a query vector to retrieve the closest matching da tabase records.\n",
      "Vectors are mathematical representations of data in a high-dimensional space. In this space, each dimension\n",
      "corresponds  to a feature of the data, and tens of thous ands of dimensions might be used to represent\n",
      "sophisticated data. A vector's position in this space represents its characteristics. Words, phrases, or entire\n",
      "documents, and images, audio, and ot her types of data can all be vectorized.[3]\n",
      "These feature vectors may be computed from the raw data using machine learning methods such as feature\n",
      "extraction algorithms, word embeddings[4] or deep learning networks. The goal is that semantically similar\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.28778567052525983\n",
      "one.io/learn/vector-database/). Pinecone. Retrieved 18 November 2023.\n",
      "2. \"What is a vector database\" (https://www.elastic.co/what-is/vector-database). Elastic.\n",
      "Retrieved 18 November 2023.\n",
      "3. \"Vector database - Azure Cosmos DB\" (https://learn.microsoft.com/en-us/azure/cosmos-db/v\n",
      "ector-database). learn.microsoft.com. 2023-12-26. Retrieved 2024-01-11.\n",
      "4. Evan Chaki (2023-07-31). \"What is a vector database?\" (https://learn.microsoft.com/en-us/se\n",
      "mantic-kernel/memories/vector-db). Microsoft. \"A vector database is a type of database that\n",
      "stores data as high-dimensional vectors, which are mathematical representations of features\n",
      "or attributes.\"\n",
      "5. \"Vector database - Azure Cosmos DB\" (https://learn.microsoft.com/en-us/azure/cosmos-db/v\n",
      "ector-database). learn.microsoft.com. 2023-12-26. Retrieved 2024-01-11.\n",
      "6. Lewis, Patrick; Perez, Ethan; Piktus, Aleksandra; Petroni, Fabio; Karpukhin, Vladimir; Goyal,\n",
      "Naman; Küttler, Heinrich (2020). \"Retrieval-augmented generation for knowledge-intensive\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.34062892198562156\n",
      "17. \"Pinecone leads 'explosion' in vector databases for generative AI\" (https://venturebeat.com/a\n",
      "i/pinecone-leads-explosion-in-vector-databases-for-generative-ai/). VentureBeat. 2023-07-\n",
      "14. Retrieved 2023-10-29.\n",
      "18. \"pgvector\" (https://github.com/pgvector/pgvector). GitHub. Retrieved 2023-11-27.\n",
      "19. \"pgvector/License\" (https://github.com/pgvector/pgvector/blob/master/LICENSE). GitHub.\n",
      "Retrieved 2023-11-27.\n",
      "20. Sawers, Paul (2023-04-19). \"Qdrant, an open source vector database startup, wants to help\n",
      "AI developers leverage unstructured data\" (https://techcrunch.com/2023/04/19/qdrant-an-ope\n",
      "n-source-vector-database-startup-wants-to-help-ai-developers-leverage-unstructured-data/).\n",
      "TechCrunch. Retrieved 2023-10-29.\n",
      "21. \"qdrant/LICENSE at master · qdrant/qdrant\" (https://github.com/qdrant/qdrant/blob/master/LIC\n",
      "ENSE). GitHub. Retrieved 2023-10-29.\n",
      "22. \"Weaviate reels in $50M for its AI-optimized vector database\" (https://siliconangle.com/2023/\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.35785118592672316\n",
      "data items receive feature vectors that are close to each other.\n",
      "Vector databases can be used for similarity search, multi-modal search, recommendations engines, large\n",
      "langua ges models (LLMs), etc.[5]\n",
      "Vector databases are also used to implement Retrieval-Augmented Generation (RAG), a method to improve\n",
      "domain-specific respons es of large language models. Text documents describing the domain of interest are\n",
      "collected and for each document a feature vector (know n as an \"embedding\") is computed, typically using a\n",
      "deep learning network, and stored in a vector database. Given a user prompt, the feature vector of the\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d4869-be21-4cf4-a72c-2e58bcc1ab43",
   "metadata": {},
   "source": [
    "## Ingesting new documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3052c81-7652-4ef0-acaf-883608a9ff85",
   "metadata": {},
   "source": [
    "#### Example with Web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "998bcc21-d03c-4889-83a6-09c62cab25eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "354cfe78-9d90-404a-8648-98fb2e79ff6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = WebBaseLoader([\"https://ai-on-openshift.io/getting-started/openshift/\",\n",
    "                        \"https://ai-on-openshift.io/getting-started/opendatahub/\",\n",
    "                        \"https://ai-on-openshift.io/getting-started/openshift-data-science/\",\n",
    "                        \"https://ai-on-openshift.io/odh-rhods/configuration/\",\n",
    "                        \"https://ai-on-openshift.io/odh-rhods/custom-notebooks/\",\n",
    "                        \"https://ai-on-openshift.io/odh-rhods/nvidia-gpus/\",\n",
    "                        \"https://ai-on-openshift.io/odh-rhods/custom-runtime-triton/\",\n",
    "                        \"https://ai-on-openshift.io/odh-rhods/openshift-group-management/\",\n",
    "                        \"https://ai-on-openshift.io/tools-and-applications/minio/minio/\"\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8ab4eaf5-d177-4410-ae9d-a012f7ffafad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "92838fe4-5b33-4835-b7e3-643ddef952c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024,\n",
    "                                               chunk_overlap=40)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "for doc in all_splits:\n",
    "    doc.page_content = doc.page_content.replace('\\x00', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ffd66d87-8314-4b2f-9c02-e856e1035e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings()\n",
    "store = PGVector(\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9d2355aa-5096-482a-ac39-4d285e63fb39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "store.add_documents(all_splits);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e9c22df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do you install OpenShift Data Science?\"\n",
    "docs_with_score = store.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4bc0bea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Score:  0.4412715768056079\n",
      "Dashboard configuration\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Custom notebooks\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    NVIDIA GPUs\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Custom Serving Runtime (Triton)\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    OpenShift Group Management\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Tools and Applications\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            Tools and Applications\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Apache Airflow\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Apache Spark\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Apache NiFi\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    MLflow\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    NVIDIA Riva\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Rclone\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Minio\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Minio\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Table of contents\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      What is it?\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Why this guide?\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Pre-requisites\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Deploying Minio on OpenShift\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Create a Data Science Project (Optional)\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Log on to your project in OpenShift Console\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Deploy Minio in your project\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Creating a bucket in Minio\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Log in to Minio\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Create a bucket\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Create a matching Data Connection for Minio\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Notes and FAQ\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Uninstall instructions:\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.44801411962256577\n",
      "MLOps with Red Hat OpenShift\n",
      "Red Hat OpenShift includes key capabilities to enable machine learning operations (MLOps) in a consistent way across datacenters, public cloud computing, and edge computing.\n",
      "By applying DevOps and GitOps principles, organizations automate and simplify the iterative process of integrating ML models into software development processes, production rollout, monitoring, retraining, and redeployment for continued prediction accuracy.\n",
      "Learn more\n",
      "What is a ML lifecycle?\n",
      "A multi-phase process to obtain the power of large volumes and a variety of data, abundant compute, and open source machine learning tools to build intelligent applications.\n",
      "At a high level, there are four steps in the lifecycle:\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.4584640636806364\n",
      "OpenShift and AI\n",
      "What is Red Hat OpenShift?\n",
      "Red Hat OpenShift brings together tested and trusted services to reduce the friction of developing, modernizing, deploying, running, and managing applications. Built on Kubernetes, it delivers a consistent experience across public cloud, on-premise, hybrid cloud, or edge architecture. Choose a self-managed or fully managed solution. No matter how you run it, OpenShift helps teams focus on the work that matters.\n",
      "Want to know more?\n",
      "Why AI on OpenShift?\n",
      "AI/ML on OpenShift accelerates AI/ML workflows and the delivery of AI-powered intelligent application.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.4834235926361533\n",
      "Deploying Minio on OpenShift\n",
      "Create a Data Science Project (Optional)\n",
      "If you already have your own Data Science Project, or OpenShift project, you can skip this step.\n",
      "\n",
      "If your cluster already has Red Hat OpenShift AI installed, you can use the Dashboard Web Interface to create a Data Science project.\n",
      "Simply navigate to Data Science Projects\n",
      "And click Create Project\n",
      "\n",
      "Choose a name for your project (here, Showcase) and click Create:\n",
      "\n",
      "\n",
      "\n",
      "Make sure to make a note of the Resource name, in case it's different from the name.\n",
      "\n",
      "\n",
      "Log on to your project in OpenShift Console\n",
      "\n",
      "\n",
      "Go to your cluster's OpenShift Console:\n",
      "\n",
      "\n",
      "\n",
      "Make sure you use the Administrator view, not the developer view.\n",
      "\n",
      "\n",
      "Go to Workloads then Pods, and confirm the selected project is the right one\n",
      "\n",
      "\n",
      "\n",
      "You now have a project in which to deploy Minio\n",
      "\n",
      "\n",
      "Deploy Minio in your project\n",
      "\n",
      "\n",
      "Click on the + (\"Import YAML\") button:\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342a9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9245b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (textgen)",
   "language": "python",
   "name": "texgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
