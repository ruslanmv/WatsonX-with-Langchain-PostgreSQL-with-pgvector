{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3420575b-4d00-458b-aa0e-7030008ccd53",
   "metadata": {},
   "source": [
    "## Creating an index and populating it with documents using PostgreSQL+pgvector\n",
    "\n",
    "Simple example on how to ingest PDF documents, then web pages content into a PostgreSQL+pgvector VectorStore.\n",
    "\n",
    "Requirements:\n",
    "- A PostgreSQL cluster with the pgvector extension installed (https://github.com/pgvector/pgvector)\n",
    "- A Database created in the cluster with the extension enabled (in this example, the database is named `vectordb`. Run the following command in the database as a superuser:\n",
    "`CREATE EXTENSION vector;`\n",
    "\n",
    "Note: if your PostgreSQL is deployed on OpenShift, directly from inside the Pod (Terminal view on the Console, or using `oc rsh` to log into the Pod), you can run the command: `psql -d vectordb -c \"CREATE EXTENSION vector;\"`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308b229-b520-4e82-a783-eb921bb955e7",
   "metadata": {},
   "source": [
    "### Needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e41b41-f60a-4b0f-91a1-cd273b60f21b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: celery 5.0.5 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8116e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: celery 5.0.5 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a82063d-6153-4812-8977-042241736b53",
   "metadata": {},
   "source": [
    "### Base parameters, the PostgreSQL info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be160a11",
   "metadata": {
    "tags": []
   },
   "source": [
    "First we need to build the CONNECTION  like:\n",
    "```python\n",
    "CONNECTION_STRING = \"postgresql+psycopg://user:password@postgresql-server:5432/vectordb\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad0096f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the values from the .env file\n",
    "user = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASSWORD\")\n",
    "database = os.getenv(\"DB_NAME\")\n",
    "server = \"af651cca01b154fe28a0df0167cad5a7-844854289.us-east-2.elb.amazonaws.com\"\n",
    "\n",
    "# Construct the connection string\n",
    "CONNECTION_STRING = f\"postgresql+psycopg://{user}:{password}@{server}:5432/{database}\"\n",
    "\n",
    "# Print the connection string\n",
    "#print(CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2734915c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (0.1.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from langchain) (2.0.20)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.14 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from langchain) (0.0.15)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.14 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from langchain) (0.1.16)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from langchain) (0.0.83)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from langchain) (1.24.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from langchain-core<0.2,>=0.1.14->langchain) (3.5.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from langchain-core<0.2,>=0.1.14->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.14->langchain) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: celery 5.0.5 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b499a49-128c-4be5-903b-76c40771c7bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "600cd763-6ecc-4c77-89c0-47108c31c44e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.pgvector import PGVector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f6785-480e-4519-be4f-8e1738dba4ca",
   "metadata": {},
   "source": [
    "## Initial index creation and document ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11fceeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (4.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: celery 5.0.5 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cff5f7-c509-48db-90b5-e15815b8b530",
   "metadata": {},
   "source": [
    "#### Document loading from a folder containing PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dde8a4a3-d602-49c6-b4a5-31a76b25a58b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_folder_path = './rhods-doc'\n",
    "loader = PyPDFDirectoryLoader(pdf_folder_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198fe0a-38bf-4cd4-af7d-35b41c645edd",
   "metadata": {},
   "source": [
    "#### Split documents into chunks with some overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edba4a08-2194-4df1-9091-6f2b596757a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Vector database\\nA vector database management system (VDBMS) or simply vector database or vector store is a\\ndatabase that can store vectors (fixed-length lists of numbers) along with other data items. Vector databases\\ntypically implement one or more Approximate Nearest Neighbor  (ANN) algorithms,[1][2] so that one can\\nsearch the database with a query vector to retrieve the closest matching da tabase records.\\nVectors are mathematical representations of data in a high-dimensional space. In this space, each dimension\\ncorresponds  to a feature of the data, and tens of thous ands of dimensions might be used to represent\\nsophisticated data. A vector's position in this space represents its characteristics. Words, phrases, or entire\\ndocuments, and images, audio, and ot her types of data can all be vectorized.[3]\\nThese feature vectors may be computed from the raw data using machine learning methods such as feature\\nextraction algorithms, word embeddings[4] or deep learning networks. The goal is that semantically similar\", metadata={'source': 'rhods-doc\\\\Vector_database.pdf', 'page': 0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024,\n",
    "                                               chunk_overlap=40)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a835e2df",
   "metadata": {},
   "source": [
    "#### Cleanup documents as PostgreSQL won't accept the NUL character, '\\x00', in TEXT fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aefc08d-a4ad-4aad-9120-cfa98b67cbe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for doc in all_splits:\n",
    "    doc.page_content = doc.page_content.replace('\\x00', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7195a2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from sentence-transformers) (4.33.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from sentence-transformers) (2.0.1+cu117)\n",
      "Requirement already satisfied: torchvision in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from sentence-transformers) (0.15.2+cu117)\n",
      "Requirement already satisfied: numpy in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from sentence-transformers) (1.24.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from sentence-transformers) (0.16.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.2)\n",
      "Requirement already satisfied: click in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from nltk->sentence-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from torchvision->sentence-transformers) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: celery 5.0.5 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37a977c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: celery 5.0.5 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: celery 5.0.5 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: celery 5.0.5 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "# Pip install necessary package\n",
    "%pip install --upgrade --quiet  langchain-openai\n",
    "%pip install --upgrade --quiet  psycopg2-binary\n",
    "%pip install --upgrade --quiet  tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae7eae2-c670-4eb5-803b-b4d591fa83db",
   "metadata": {},
   "source": [
    "#### Create the index and ingest the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c056c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg\n",
      "  Obtaining dependency information for psycopg from https://files.pythonhosted.org/packages/fe/f2/ab7de9bed559fa1f5efe2b9638be6e2d51ae605c9c5a321e26290cfe9899/psycopg-3.1.17-py3-none-any.whl.metadata\n",
      "  Using cached psycopg-3.1.17-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from psycopg) (4.7.1)\n",
      "Requirement already satisfied: tzdata in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from psycopg) (2023.3)\n",
      "Using cached psycopg-3.1.17-py3-none-any.whl (178 kB)\n",
      "Installing collected packages: psycopg\n",
      "Successfully installed psycopg-3.1.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: celery 5.0.5 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62c084b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pq\n",
      "  Downloading pq-1.9.1.tar.gz (15 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: pq\n",
      "  Building wheel for pq (setup.py): started\n",
      "  Building wheel for pq (setup.py): finished with status 'done'\n",
      "  Created wheel for pq: filename=pq-1.9.1-py3-none-any.whl size=12563 sha256=4ff6602fb860f3b539f77c917247e8b07e020490de4f03839f51015a61786bc3\n",
      "  Stored in directory: c:\\users\\rusla\\appdata\\local\\pip\\cache\\wheels\\ae\\88\\f0\\0e3e6cbc020914476fd134c58e2b1a336bf7afea9559e217c5\n",
      "Successfully built pq\n",
      "Installing collected packages: pq\n",
      "Successfully installed pq-1.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: celery 5.0.5 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "pip install pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47cbf017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg[binary] in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (3.1.17)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from psycopg[binary]) (4.7.1)\n",
      "Requirement already satisfied: tzdata in c:\\users\\rusla\\.conda\\envs\\textgen\\lib\\site-packages (from psycopg[binary]) (2023.3)\n",
      "Collecting psycopg-binary==3.1.17 (from psycopg[binary])\n",
      "  Obtaining dependency information for psycopg-binary==3.1.17 from https://files.pythonhosted.org/packages/97/5e/4443a7a19e02486026d9556ac063b98c4185c3037c327d5d64e819b9bca7/psycopg_binary-3.1.17-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading psycopg_binary-3.1.17-cp310-cp310-win_amd64.whl.metadata (2.9 kB)\n",
      "Downloading psycopg_binary-3.1.17-cp310-cp310-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.9 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.1/2.9 MB 26.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.1/2.9 MB 26.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/2.9 MB 16.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 14.2 MB/s eta 0:00:00\n",
      "Installing collected packages: psycopg-binary\n",
      "Successfully installed psycopg-binary-3.1.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: celery 5.0.5 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install \"psycopg[binary]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbb6a3e3-5ccd-441e-b80d-427555d9e9f6",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "COLLECTION_NAME = \"documents_test\"\n",
    "\n",
    "db = PGVector.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca4ca1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is vector database?\"\n",
    "docs_with_score = db.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8437c826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Score:  0.18316362945708264\n",
      "Vector database\n",
      "A vector database management system (VDBMS) or simply vector database or vector store is a\n",
      "database that can store vectors (fixed-length lists of numbers) along with other data items. Vector databases\n",
      "typically implement one or more Approximate Nearest Neighbor  (ANN) algorithms,[1][2] so that one can\n",
      "search the database with a query vector to retrieve the closest matching da tabase records.\n",
      "Vectors are mathematical representations of data in a high-dimensional space. In this space, each dimension\n",
      "corresponds  to a feature of the data, and tens of thous ands of dimensions might be used to represent\n",
      "sophisticated data. A vector's position in this space represents its characteristics. Words, phrases, or entire\n",
      "documents, and images, audio, and ot her types of data can all be vectorized.[3]\n",
      "These feature vectors may be computed from the raw data using machine learning methods such as feature\n",
      "extraction algorithms, word embeddings[4] or deep learning networks. The goal is that semantically similar\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.2877857856622794\n",
      "one.io/learn/vector-database/). Pinecone. Retrieved 18 November 2023.\n",
      "2. \"What is a vector database\" (https://www.elastic.co/what-is/vector-database). Elastic.\n",
      "Retrieved 18 November 2023.\n",
      "3. \"Vector database - Azure Cosmos DB\" (https://learn.microsoft.com/en-us/azure/cosmos-db/v\n",
      "ector-database). learn.microsoft.com. 2023-12-26. Retrieved 2024-01-11.\n",
      "4. Evan Chaki (2023-07-31). \"What is a vector database?\" (https://learn.microsoft.com/en-us/se\n",
      "mantic-kernel/memories/vector-db). Microsoft. \"A vector database is a type of database that\n",
      "stores data as high-dimensional vectors, which are mathematical representations of features\n",
      "or attributes.\"\n",
      "5. \"Vector database - Azure Cosmos DB\" (https://learn.microsoft.com/en-us/azure/cosmos-db/v\n",
      "ector-database). learn.microsoft.com. 2023-12-26. Retrieved 2024-01-11.\n",
      "6. Lewis, Patrick; Perez, Ethan; Piktus, Aleksandra; Petroni, Fabio; Karpukhin, Vladimir; Goyal,\n",
      "Naman; Küttler, Heinrich (2020). \"Retrieval-augmented generation for knowledge-intensive\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.34062906149798877\n",
      "17. \"Pinecone leads 'explosion' in vector databases for generative AI\" (https://venturebeat.com/a\n",
      "i/pinecone-leads-explosion-in-vector-databases-for-generative-ai/). VentureBeat. 2023-07-\n",
      "14. Retrieved 2023-10-29.\n",
      "18. \"pgvector\" (https://github.com/pgvector/pgvector). GitHub. Retrieved 2023-11-27.\n",
      "19. \"pgvector/License\" (https://github.com/pgvector/pgvector/blob/master/LICENSE). GitHub.\n",
      "Retrieved 2023-11-27.\n",
      "20. Sawers, Paul (2023-04-19). \"Qdrant, an open source vector database startup, wants to help\n",
      "AI developers leverage unstructured data\" (https://techcrunch.com/2023/04/19/qdrant-an-ope\n",
      "n-source-vector-database-startup-wants-to-help-ai-developers-leverage-unstructured-data/).\n",
      "TechCrunch. Retrieved 2023-10-29.\n",
      "21. \"qdrant/LICENSE at master · qdrant/qdrant\" (https://github.com/qdrant/qdrant/blob/master/LIC\n",
      "ENSE). GitHub. Retrieved 2023-10-29.\n",
      "22. \"Weaviate reels in $50M for its AI-optimized vector database\" (https://siliconangle.com/2023/\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.35785103063443546\n",
      "data items receive feature vectors that are close to each other.\n",
      "Vector databases can be used for similarity search, multi-modal search, recommendations engines, large\n",
      "langua ges models (LLMs), etc.[5]\n",
      "Vector databases are also used to implement Retrieval-Augmented Generation (RAG), a method to improve\n",
      "domain-specific respons es of large language models. Text documents describing the domain of interest are\n",
      "collected and for each document a feature vector (know n as an \"embedding\") is computed, typically using a\n",
      "deep learning network, and stored in a vector database. Given a user prompt, the feature vector of the\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d4869-be21-4cf4-a72c-2e58bcc1ab43",
   "metadata": {},
   "source": [
    "## Ingesting new documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3052c81-7652-4ef0-acaf-883608a9ff85",
   "metadata": {},
   "source": [
    "#### Example with Web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "998bcc21-d03c-4889-83a6-09c62cab25eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "354cfe78-9d90-404a-8648-98fb2e79ff6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = WebBaseLoader([\"https://ai-on-openshift.io/getting-started/openshift/\",\n",
    "                        \"https://ai-on-openshift.io/getting-started/opendatahub/\",\n",
    "                        \"https://ai-on-openshift.io/getting-started/openshift-data-science/\",\n",
    "                        \"https://ai-on-openshift.io/odh-rhods/configuration/\",\n",
    "                        \"https://ai-on-openshift.io/odh-rhods/custom-notebooks/\",\n",
    "                        \"https://ai-on-openshift.io/odh-rhods/nvidia-gpus/\",\n",
    "                        \"https://ai-on-openshift.io/odh-rhods/custom-runtime-triton/\",\n",
    "                        \"https://ai-on-openshift.io/odh-rhods/openshift-group-management/\",\n",
    "                        \"https://ai-on-openshift.io/tools-and-applications/minio/minio/\"\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ab4eaf5-d177-4410-ae9d-a012f7ffafad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92838fe4-5b33-4835-b7e3-643ddef952c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024,\n",
    "                                               chunk_overlap=40)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "for doc in all_splits:\n",
    "    doc.page_content = doc.page_content.replace('\\x00', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ffd66d87-8314-4b2f-9c02-e856e1035e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings()\n",
    "store = PGVector(\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9d2355aa-5096-482a-ac39-4d285e63fb39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "store.add_documents(all_splits);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9c22df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do you install OpenShift Data Science?\"\n",
    "docs_with_score = store.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4bc0bea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Score:  0.44127142429351807\n",
      "Dashboard configuration\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Custom notebooks\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    NVIDIA GPUs\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Custom Serving Runtime (Triton)\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    OpenShift Group Management\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Tools and Applications\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            Tools and Applications\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Apache Airflow\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Apache Spark\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Apache NiFi\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    MLflow\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    NVIDIA Riva\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Rclone\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Minio\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Minio\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Table of contents\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      What is it?\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Why this guide?\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Pre-requisites\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Deploying Minio on OpenShift\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Create a Data Science Project (Optional)\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Log on to your project in OpenShift Console\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Deploy Minio in your project\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Creating a bucket in Minio\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Log in to Minio\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Create a bucket\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Create a matching Data Connection for Minio\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Notes and FAQ\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Uninstall instructions:\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.44127142429351807\n",
      "Dashboard configuration\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Custom notebooks\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    NVIDIA GPUs\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Custom Serving Runtime (Triton)\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    OpenShift Group Management\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Tools and Applications\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            Tools and Applications\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Apache Airflow\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Apache Spark\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Apache NiFi\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    MLflow\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    NVIDIA Riva\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Rclone\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Minio\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Minio\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Table of contents\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      What is it?\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Why this guide?\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Pre-requisites\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Deploying Minio on OpenShift\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Create a Data Science Project (Optional)\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Log on to your project in OpenShift Console\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Deploy Minio in your project\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Creating a bucket in Minio\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Log in to Minio\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Create a bucket\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Create a matching Data Connection for Minio\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Notes and FAQ\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Uninstall instructions:\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.44127142429351807\n",
      "Dashboard configuration\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Custom notebooks\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    NVIDIA GPUs\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Custom Serving Runtime (Triton)\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    OpenShift Group Management\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Tools and Applications\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            Tools and Applications\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Apache Airflow\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Apache Spark\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Apache NiFi\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    MLflow\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    NVIDIA Riva\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Rclone\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Minio\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Minio\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Table of contents\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      What is it?\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Why this guide?\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Pre-requisites\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Deploying Minio on OpenShift\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Create a Data Science Project (Optional)\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Log on to your project in OpenShift Console\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Deploy Minio in your project\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Creating a bucket in Minio\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Log in to Minio\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Create a bucket\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Create a matching Data Connection for Minio\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Notes and FAQ\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Uninstall instructions:\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.4480140209197998\n",
      "MLOps with Red Hat OpenShift\n",
      "Red Hat OpenShift includes key capabilities to enable machine learning operations (MLOps) in a consistent way across datacenters, public cloud computing, and edge computing.\n",
      "By applying DevOps and GitOps principles, organizations automate and simplify the iterative process of integrating ML models into software development processes, production rollout, monitoring, retraining, and redeployment for continued prediction accuracy.\n",
      "Learn more\n",
      "What is a ML lifecycle?\n",
      "A multi-phase process to obtain the power of large volumes and a variety of data, abundant compute, and open source machine learning tools to build intelligent applications.\n",
      "At a high level, there are four steps in the lifecycle:\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342a9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (textgen)",
   "language": "python",
   "name": "texgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
